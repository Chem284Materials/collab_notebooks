{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apxLoGAqT71k",
        "outputId": "ae9bb9ff-d25c-4e7c-ad47-405c8533adda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping cuda-bindings as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping cuda-python as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting cuda-bindings==12.8.0\n",
            "  Downloading cuda_bindings-12.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting cuda-python==12.8.0\n",
            "  Downloading cuda_python-12.8.0-py3-none-any.whl.metadata (15 kB)\n",
            "Downloading cuda_bindings-12.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cuda_python-12.8.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: cuda-bindings, cuda-python\n",
            "Successfully installed cuda-bindings-12.8.0 cuda-python-12.8.0\n"
          ]
        }
      ],
      "source": [
        "# Downgrade both to keep them in lockstep\n",
        "!python -m pip uninstall -y cuda-bindings cuda-python\n",
        "!python -m pip install --no-cache-dir \"cuda-bindings==12.8.0\" \"cuda-python==12.8.0\"\n",
        "!python -m pip install --no-cache-dir nvidia-cutlass-dsl pycuda\n",
        "\n",
        "# Then restart the runtime (Runtime → Restart runtime) and run:\n",
        "# import cutlass.cute\n",
        "# import cuda.bindings.driver as cu; cu.cuInit(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cutlass.cute\n",
        "import cuda.bindings.driver as cu; cu.cuInit(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hWMSq4tUKdx",
        "outputId": "c6dd82a6-c185-493b-f18d-1717939867d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<CUresult.CUDA_SUCCESS: 0>,)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cutlass\n",
        "import cutlass.cute as cute\n",
        "from cutlass.cute.runtime import from_dlpack\n",
        "import cupy as cp\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "@cute.kernel\n",
        "def elementwise_add_kernel(A, B, C):\n",
        "    tidx, _, _ = cute.arch.thread_idx()\n",
        "    bidx, _, _ = cute.arch.block_idx()\n",
        "    bdim, _, _ = cute.arch.block_dim()\n",
        "\n",
        "    global_x = bidx * bdim + tidx\n",
        "\n",
        "    nx, ny = A.shape\n",
        "    ix = global_x % nx;\n",
        "    iy = global_x // nx;\n",
        "\n",
        "    C[ix, iy] = A[ix, iy] + B[ix, iy]\n",
        "    #C[0,0] = 1.23\n",
        "    #C[ix, iy] = 123.0\n",
        "\n",
        "@cute.jit\n",
        "def elementwise_add(A, B, C):\n",
        "    num_threads_per_block = 256\n",
        "    nx, ny = A.shape\n",
        "    kernel = elementwise_add_kernel(A, B, C)\n",
        "    kernel.launch(\n",
        "            grid=((nx * ny) // num_threads_per_block, 1, 1),\n",
        "            block=(num_threads_per_block, 1, 1)\n",
        "            )\n",
        "\n",
        "    #cute.print_tensor(A)\n",
        "    #cute.print_tensor(B)\n",
        "    #cute.print_tensor(C)\n",
        "\n",
        "Nx = 2048\n",
        "Ny = 2048\n",
        "A = cp.random.uniform(0.0, 1.0, (Nx, Ny)).astype(cp.float32)\n",
        "B = cp.random.uniform(0.0, 1.0, (Nx, Ny)).astype(cp.float32)\n",
        "C = cp.empty( (Nx, Ny), dtype=cp.float32)\n",
        "\n",
        "#A_ = from_dlpack(A, assumed_align=32)\n",
        "#B_ = from_dlpack(B, assumed_align=32)\n",
        "#C_ = from_dlpack(C, assumed_align=32)\n",
        "A_ = from_dlpack(A)\n",
        "B_ = from_dlpack(B)\n",
        "C_ = from_dlpack(C)\n",
        "\n",
        "cutlass.cuda.initialize_cuda_context()\n",
        "start_time = time.time()\n",
        "for i in range(100):\n",
        "    elementwise_add(A_, B_, C_)\n",
        "print(f\"Total time: {time.time()-start_time}\")\n",
        "\n",
        "\n",
        "print(\"C[0:3, 0:3] =\\n\", cp.asnumpy(C[:3, :3]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sp9HwcIWI5N",
        "outputId": "126c7710-1f3e-4b43-89f4-8795cebd34eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time: 1.4251413345336914\n",
            "C[0:3, 0:3] =\n",
            " [[1.3828324  0.5965689  1.2344048 ]\n",
            " [0.7937813  0.8457347  0.80628765]\n",
            " [1.0233462  1.1294141  0.8820095 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cutlass\n",
        "import cutlass.cute as cute\n",
        "from cutlass.cute.runtime import from_dlpack\n",
        "import cupy as cp\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "@cute.kernel\n",
        "def elementwise_add_kernel(A, B, C):\n",
        "    tidx, _, _ = cute.arch.thread_idx()\n",
        "    bidx, _, _ = cute.arch.block_idx()\n",
        "    bdim, _, _ = cute.arch.block_dim()\n",
        "\n",
        "    global_x = bidx * bdim + tidx\n",
        "\n",
        "    #nx, ny = A.shape\n",
        "    #ix = global_x % nx;\n",
        "    #iy = global_x // nx;\n",
        "\n",
        "    #C[ix, iy] = A[ix, iy] + B[ix, iy]\n",
        "\n",
        "    # Map thread index to logical index of input tensor\n",
        "    m, n = A.shape[1]       # thread-domain\n",
        "    ni = global_x % n\n",
        "    mi = global_x // n\n",
        "\n",
        "    # Map logical index to physical address via tensor layout\n",
        "    a_val = A[(None, (mi, ni))].load()\n",
        "    b_val = B[(None, (mi, ni))].load()\n",
        "    #print(f\"[DSL INFO] sliced gA = {A[(None, (mi, ni))]}\")\n",
        "    #print(f\"[DSL INFO] sliced gB = {B[(None, (mi, ni))]}\")\n",
        "\n",
        "    # Perform element-wise addition\n",
        "    C[(None, (mi, ni))] = a_val + b_val\n",
        "\n",
        "\n",
        "@cute.jit\n",
        "def elementwise_add(A, B, C):\n",
        "    num_threads_per_block = 256\n",
        "\n",
        "    gA = cute.zipped_divide(A, (1, 4))\n",
        "    gB = cute.zipped_divide(B, (1, 4))\n",
        "    gC = cute.zipped_divide(C, (1, 4))\n",
        "\n",
        "    #print(f\"[DSL INFO] Tiled Tensors:\")\n",
        "    #print(f\"[DSL INFO]   gA = {gA}\")\n",
        "    #print(f\"[DSL INFO]   gB = {gB}\")\n",
        "    #print(f\"[DSL INFO]   gC = {gC}\")\n",
        "\n",
        "    elementwise_add_kernel(gA, gB, gC).launch(\n",
        "        grid=(cute.size(gC, mode=[1]) // num_threads_per_block, 1, 1),\n",
        "        block=(num_threads_per_block, 1, 1),\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "Nx = 2048\n",
        "Ny = 2048\n",
        "A = cp.random.uniform(0.0, 1.0, (Nx, Ny)).astype(cp.float32)\n",
        "B = cp.random.uniform(0.0, 1.0, (Nx, Ny)).astype(cp.float32)\n",
        "C = cp.empty( (Nx, Ny), dtype=cp.float32)\n",
        "\n",
        "A_ = from_dlpack(A)\n",
        "B_ = from_dlpack(B)\n",
        "C_ = from_dlpack(C)\n",
        "\n",
        "cutlass.cuda.initialize_cuda_context()\n",
        "start_time = time.time()\n",
        "for i in range(100):\n",
        "    elementwise_add(A_, B_, C_)\n",
        "print(f\"Total time: {time.time()-start_time}\")\n",
        "\n",
        "\n",
        "print(\"C[0:3, 0:3] =\\n\", cp.asnumpy(C[:3, :3]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYVNAD4NV1IY",
        "outputId": "1b429fd2-f7bf-4d33-84ed-a3d3094e4d6b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time: 1.9427759647369385\n",
            "C[0:3, 0:3] =\n",
            " [[-8.9898463e+28  1.8275932e+00 -1.1929849e-14]\n",
            " [ 1.9503593e+20  1.6870652e+00  3.5662110e-38]\n",
            " [-1.0447428e+01  1.7868803e+00 -7.1491721e+37]]\n"
          ]
        }
      ]
    }
  ]
}