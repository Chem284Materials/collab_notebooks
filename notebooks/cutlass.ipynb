{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-OZcKQPJFic"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth=1 https://github.com/NVIDIA/cutlass.git"
      ],
      "metadata": {
        "id": "m4d6rGQrJWew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "gpu_name=\"$(nvidia-smi --query-gpu=name --format=csv,noheader | head -n1)\"\n",
        "\n",
        "# Default to a safe-ish arch; override below when we recognize the GPU.\n",
        "sm=70\n",
        "\n",
        "case \"$gpu_name\" in\n",
        "  *\"A100\"*) sm=80 ;;\n",
        "  *\"H100\"*) sm=90 ;;   # Hopper\n",
        "  *\"L4\"*)   sm=89 ;;\n",
        "  *\"A10\"*)  sm=86 ;;\n",
        "  *\"RTX 30\"*\"A40\"*|\"*A40*\") sm=86 ;;\n",
        "  *\"V100\"*) sm=70 ;;\n",
        "  *\"T4\"*)   sm=75 ;;\n",
        "  *\"P100\"*) sm=60 ;;\n",
        "esac\n",
        "\n",
        "echo \"Detected GPU: $gpu_name\"\n",
        "echo \"Using -arch=sm_${sm}\"\n",
        "\n",
        "# Save for later cells\n",
        "echo \"SM=${sm}\" > sm.env\n"
      ],
      "metadata": {
        "id": "XKFa-jy0Jbzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "cat > cutlass_gemm.cu <<'CU'\n",
        "#include <cstdio>\n",
        "#include <vector>\n",
        "#include <random>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#include \"cutlass/cutlass.h\"\n",
        "#include \"cutlass/gemm/device/gemm.h\"\n",
        "#include \"cutlass/layout/matrix.h\"\n",
        "\n",
        "#define CHECK_CUDA(call) do { \\\n",
        "  cudaError_t status = (call); \\\n",
        "  if (status != cudaSuccess) { \\\n",
        "    printf(\"CUDA Error %s at %s:%d\\n\", cudaGetErrorString(status), __FILE__, __LINE__); \\\n",
        "    return -1; \\\n",
        "  } \\\n",
        "} while(0)\n",
        "\n",
        "int main() {\n",
        "  using Element = float;\n",
        "  using Layout = cutlass::layout::RowMajor;\n",
        "\n",
        "  // GEMM: C[M,N] = alpha * A[M,K] * B[K,N] + beta * C[M,N]\n",
        "  int M = 1024, N = 1024, K = 1024;\n",
        "  Element alpha = 1.0f, beta = 0.0f;\n",
        "\n",
        "  // Host buffers\n",
        "  std::vector<Element> hA(M*K), hB(K*N), hC(M*N);\n",
        "\n",
        "  // Fill A and B with random values\n",
        "  std::mt19937 rng(123);\n",
        "  std::uniform_real_distribution<float> dist(-1.f, 1.f);\n",
        "  for (auto &x : hA) x = dist(rng);\n",
        "  for (auto &x : hB) x = dist(rng);\n",
        "  std::fill(hC.begin(), hC.end(), 0.0f);\n",
        "\n",
        "  // Device buffers\n",
        "  Element *dA=nullptr, *dB=nullptr, *dC=nullptr;\n",
        "  CHECK_CUDA(cudaMalloc((void**)&dA, sizeof(Element)*hA.size()));\n",
        "  CHECK_CUDA(cudaMalloc((void**)&dB, sizeof(Element)*hB.size()));\n",
        "  CHECK_CUDA(cudaMalloc((void**)&dC, sizeof(Element)*hC.size()));\n",
        "\n",
        "  CHECK_CUDA(cudaMemcpy(dA, hA.data(), sizeof(Element)*hA.size(), cudaMemcpyHostToDevice));\n",
        "  CHECK_CUDA(cudaMemcpy(dB, hB.data(), sizeof(Element)*hB.size(), cudaMemcpyHostToDevice));\n",
        "  CHECK_CUDA(cudaMemcpy(dC, hC.data(), sizeof(Element)*hC.size(), cudaMemcpyHostToDevice));\n",
        "\n",
        "  // CUTLASS GEMM type (FP32, row-major)\n",
        "  using Gemm = cutlass::gemm::device::Gemm<\n",
        "      Element, Layout,   // A\n",
        "      Element, Layout,   // B\n",
        "      Element, Layout    // C/D\n",
        "  >;\n",
        "\n",
        "  Gemm gemm_op;\n",
        "\n",
        "  Gemm::Arguments args(\n",
        "      {M, N, K},                 // Problem size\n",
        "      {dA, K},                   // A ptr, lda\n",
        "      {dB, N},                   // B ptr, ldb\n",
        "      {dC, N},                   // C ptr, ldc\n",
        "      {dC, N},                   // D ptr, ldd (output)\n",
        "      {alpha, beta}\n",
        "  );\n",
        "\n",
        "  // Optional: workspace (some kernels require it)\n",
        "  size_t workspace_size = Gemm::get_workspace_size(args);\n",
        "  void* workspace = nullptr;\n",
        "  if (workspace_size) CHECK_CUDA(cudaMalloc(&workspace, workspace_size));\n",
        "\n",
        "  cutlass::Status status = gemm_op.can_implement(args);\n",
        "  if (status != cutlass::Status::kSuccess) {\n",
        "    printf(\"GEMM configuration not supported.\\n\");\n",
        "    return -1;\n",
        "  }\n",
        "\n",
        "  status = gemm_op.initialize(args, workspace);\n",
        "  if (status != cutlass::Status::kSuccess) {\n",
        "    printf(\"GEMM initialize failed.\\n\");\n",
        "    return -1;\n",
        "  }\n",
        "\n",
        "  status = gemm_op();\n",
        "  if (status != cutlass::Status::kSuccess) {\n",
        "    printf(\"GEMM run failed.\\n\");\n",
        "    return -1;\n",
        "  }\n",
        "\n",
        "  CHECK_CUDA(cudaMemcpy(hC.data(), dC, sizeof(Element)*hC.size(), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // Simple checksum so we know it worked\n",
        "  double checksum = 0.0;\n",
        "  for (int i = 0; i < 10; ++i) checksum += hC[i];\n",
        "  printf(\"Done. C[0..9] sum = %.6f\\n\", checksum);\n",
        "\n",
        "  if (workspace) cudaFree(workspace);\n",
        "  cudaFree(dA); cudaFree(dB); cudaFree(dC);\n",
        "  return 0;\n",
        "}\n",
        "CU\n",
        "\n",
        "# Read saved SM from earlier step\n",
        "source sm.env\n",
        "\n",
        "# Common NVCC flags for CUTLASS on Colab; allow-unsupported handles frequent host-compiler versions\n",
        "NVCCFLAGS=\"-O3 -std=c++17 --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -Wno-deprecated-declarations --allow-unsupported-compiler\"\n",
        "\n",
        "# Build: include CUTLASS headers\n",
        "nvcc cutlass_gemm.cu -I cutlass/include -arch=sm_${SM} ${NVCCFLAGS} -o cutlass_gemm\n"
      ],
      "metadata": {
        "id": "NO54mz5bJki6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cutlass_gemm\n"
      ],
      "metadata": {
        "id": "u8L8qOCjJtyF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}